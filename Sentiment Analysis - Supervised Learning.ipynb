{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in all book reviews\n",
    "\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "books = [\"Andy-Weir-The-Martian.csv\", \"Donna-Tartt-The-Goldfinch.csv\", \n",
    "         \"EL-James-Fifty-Shades-of-Grey.csv\", \"Fillian_Flynn-Gone_Girl.csv\", \n",
    "         \"John-Green-The-Fault-in-our-Stars.csv\", \"Laura-Hillenbrand-Unbroken.csv\", \n",
    "         \"Paula_Hawkins-The-Girl-On-The-Train.csv\", \"Suzanne-Collins-The-Hunger-Games.csv\"]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separate each book review into different classes - pos, neutral, neg [neutral class not dealt with this during this round]\n",
    "\n",
    "neg = []\n",
    "neutral = []\n",
    "pos = []\n",
    "\n",
    "for i in books:  \n",
    "    for line in open(i):\n",
    "        if int(line[0]) == 1 or int(line[0]) == 2:\n",
    "            neg.append(line)\n",
    "        elif int(line[0]) == 3:\n",
    "            neutral.append(line)\n",
    "        else:\n",
    "            pos.append(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.0\\t/gp/customer-reviews/RKMO449VT48H3?ASIN=1491590173\\t4.7573214851 Stars\\t\"<span class=\"\"a-size-base review-text\"\">I\\'m a hard-science science fiction fan and would rather read hard sc-fi than almost anything. I love stories and movies about Mars, and I\\'m a fan of survival, castaway, and man-against-the elements stories. I loved Robinson Crusoe, so it should not surprise you that I loved the movie, Robinson Crusoe on Mars. I realize it\\'s not Academy Award material, but to me, it\\'s everything I want it to be, as was this book, The Martian.<br/><br/>The main character, Watney, presumed dead, is accidentally left by his crew mates when an intense Martian dust storm forces them to abort their mission. What follows for part of the book is a logbook style narrative that describes in great technical detail Watney\\'s efforts to extend his life until the next scheduled mission arrives in 4 years. After reading just the first 20% of the book (my Kindle has no page numbers) one can\\'t help but be impressed by the author\\'s depth of knowledge in this regard. In fact, the entire book is an astronaut\\'s primer on extraterrestrial and deep space survival and rescue.<br/><br/>The Martian isn\\'t without its typos and editorial glitches, and I\\'m not sure if this was a result of a bad Kindle conversion or just a shortsighted editor. For me, though, typos and editing issues paled in comparison to the snowballing storyline, which I gladly admit is not for everyone.<br/><br/>This is not a touchy-feely book about love, romance or relationships. There is no overpowering angle between characters. No good guys in white hats and bad guys in black hats. There\\'s no room for cliches. It\\'s all very business like and scientific. So, if you\\'re looking for Twilight in Space. Or Fifty Shades of Mars. Or Tom Hanks making himself a friend by drawing a face on a soccer ball, you\\'ll probably want to skip this one. This book is simply about the mission, and the cold reality of working hard to turn a wrong into a right.<br/><br/>Another thing you won\\'t find in this book is a lot of heartfelt reminiscing or reflection. There are no flashbacks of our main character fishing with Dad at the old water hole, or him riding his first bicycle without training wheels. This is a book about a guy with a keen intellect surviving on a hostile planet and doing so by making the most out of a given set of resources.<br/><br/>About a third of the way through the book, the author adds third person narratives from mission control and the Hermes space craft, the latter manned by the crew that left our hero behind -- and make no mistake, hero is the operative word. Again, we don\\'t follow our mission control cast of characters back too their respective homes and meet their wives and husbands and get served up cliche insights into their innermost thoughts. Blech! I hate those stories! Which doesn\\'t mean these characters are cookie cutter or superficial. On the contrary, I found the characters sufficiently individuated and interesting.<br/><br/>I highly recommend this book to people who are into reading hard sci-fi of the not-too-distant future, sci-fi without blasters and ray guns or 9\\' tall aliens that bleed acid. (Btw, I like those stories, too, but good ones are hard to find.)<br/><br/>Somebody did their homework on this one -- and that\\'s what stands out above all else.</span>\"\\r\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'m\", 'a', 'hard-science', 'science', 'fiction', 'fan', 'and', 'would', 'rather', 'read', 'hard', 'sc-fi', 'than', 'almost', 'anything', '.', 'i', 'love', 'stories', 'and', 'movies', 'about', 'mars', ',', 'and', 'i', \"'m\", 'a', 'fan', 'of', 'survival', ',', 'castaway', ',', 'and', 'man-against-the', 'elements', 'stories', '.', 'i', 'loved', 'robinson', 'crusoe', ',', 'so', 'it', 'should', 'not', 'surprise', 'you', 'that', 'i', 'loved', 'the', 'movie', ',', 'robinson', 'crusoe', 'on', 'mars', '.', 'i', 'realize', 'it', \"'s\", 'not', 'academy', 'award', 'material', ',', 'but', 'to', 'me', ',', 'it', \"'s\", 'everything', 'i', 'want', 'it', 'to', 'be', ',', 'as', 'was', 'this', 'book', ',', 'the', 'martian', '.', 'the', 'main', 'character', ',', 'watney', ',', 'presumed', 'dead', ',', 'is', 'accidentally', 'left', 'by', 'his', 'crew', 'mates', 'when', 'an', 'intense', 'martian', 'dust', 'storm', 'forces', 'them', 'to', 'abort', 'their', 'mission', '.', 'what', 'follows', 'for', 'part', 'of', 'the', 'book', 'is', 'a', 'logbook', 'style', 'narrative', 'that', 'describes', 'in', 'great', 'technical', 'detail', 'watney', \"'s\", 'efforts', 'to', 'extend', 'his', 'life', 'until', 'the', 'next', 'scheduled', 'mission', 'arrives', 'in', '4', 'years', '.', 'after', 'reading', 'just', 'the', 'first', '20', '%', 'of', 'the', 'book', '(', 'my', 'kindle', 'has', 'no', 'page', 'numbers', ')', 'one', 'ca', \"n't\", 'help', 'but', 'be', 'impressed', 'by', 'the', 'author', \"'s\", 'depth', 'of', 'knowledge', 'in', 'this', 'regard', '.', 'in', 'fact', ',', 'the', 'entire', 'book', 'is', 'an', 'astronaut', \"'s\", 'primer', 'on', 'extraterrestrial', 'and', 'deep', 'space', 'survival', 'and', 'rescue', '.', 'the', 'martian', 'is', \"n't\", 'without', 'its', 'typos', 'and', 'editorial', 'glitches', ',', 'and', 'i', \"'m\", 'not', 'sure', 'if', 'this', 'was', 'a', 'result', 'of', 'a', 'bad', 'kindle', 'conversion', 'or', 'just', 'a', 'shortsighted', 'editor', '.', 'for', 'me', ',', 'though', ',', 'typos', 'and', 'editing', 'issues', 'paled', 'in', 'comparison', 'to', 'the', 'snowballing', 'storyline', ',', 'which', 'i', 'gladly', 'admit', 'is', 'not', 'for', 'everyone', '.', 'this', 'is', 'not', 'a', 'touchy-feely', 'book', 'about', 'love', ',', 'romance', 'or', 'relationships', '.', 'there', 'is', 'no', 'overpowering', 'angle', 'between', 'characters', '.', 'no', 'good', 'guys', 'in', 'white', 'hats', 'and', 'bad', 'guys', 'in', 'black', 'hats', '.', 'there', \"'s\", 'no', 'room', 'for', 'cliches', '.', 'it', \"'s\", 'all', 'very', 'business', 'like', 'and', 'scientific', '.', 'so', ',', 'if', 'you', \"'re\", 'looking', 'for', 'twilight', 'in', 'space', '.', 'or', 'fifty', 'shades', 'of', 'mars', '.', 'or', 'tom', 'hanks', 'making', 'himself', 'a', 'friend', 'by', 'drawing', 'a', 'face', 'on', 'a', 'soccer', 'ball', ',', 'you', \"'ll\", 'probably', 'want', 'to', 'skip', 'this', 'one', '.', 'this', 'book', 'is', 'simply', 'about', 'the', 'mission', ',', 'and', 'the', 'cold', 'reality', 'of', 'working', 'hard', 'to', 'turn', 'a', 'wrong', 'into', 'a', 'right', '.', 'another', 'thing', 'you', 'wo', \"n't\", 'find', 'in', 'this', 'book', 'is', 'a', 'lot', 'of', 'heartfelt', 'reminiscing', 'or', 'reflection', '.', 'there', 'are', 'no', 'flashbacks', 'of', 'our', 'main', 'character', 'fishing', 'with', 'dad', 'at', 'the', 'old', 'water', 'hole', ',', 'or', 'him', 'riding', 'his', 'first', 'bicycle', 'without', 'training', 'wheels', '.', 'this', 'is', 'a', 'book', 'about', 'a', 'guy', 'with', 'a', 'keen', 'intellect', 'surviving', 'on', 'a', 'hostile', 'planet', 'and', 'doing', 'so', 'by', 'making', 'the', 'most', 'out', 'of', 'a', 'given', 'set', 'of', 'resources', '.', 'about', 'a', 'third', 'of', 'the', 'way', 'through', 'the', 'book', ',', 'the', 'author', 'adds', 'third', 'person', 'narratives', 'from', 'mission', 'control', 'and', 'the', 'hermes', 'space', 'craft', ',', 'the', 'latter', 'manned', 'by', 'the', 'crew', 'that', 'left', 'our', 'hero', 'behind', '--', 'and', 'make', 'no', 'mistake', ',', 'hero', 'is', 'the', 'operative', 'word', '.', 'again', ',', 'we', 'do', \"n't\", 'follow', 'our', 'mission', 'control', 'cast', 'of', 'characters', 'back', 'too', 'their', 'respective', 'homes', 'and', 'meet', 'their', 'wives', 'and', 'husbands', 'and', 'get', 'served', 'up', 'cliche', 'insights', 'into', 'their', 'innermost', 'thoughts', '.', 'blech', '!', 'i', 'hate', 'those', 'stories', '!', 'which', 'does', \"n't\", 'mean', 'these', 'characters', 'are', 'cookie', 'cutter', 'or', 'superficial', '.', 'on', 'the', 'contrary', ',', 'i', 'found', 'the', 'characters', 'sufficiently', 'individuated', 'and', 'interesting', '.', 'i', 'highly', 'recommend', 'this', 'book', 'to', 'people', 'who', 'are', 'into', 'reading', 'hard', 'sci-fi', 'of', 'the', 'not-too-distant', 'future', ',', 'sci-fi', 'without', 'blasters', 'and', 'ray', 'guns', 'or', '9', \"'\", 'tall', 'aliens', 'that', 'bleed', 'acid', '.', '(', 'btw', ',', 'i', 'like', 'those', 'stories', ',', 'too', ',', 'but', 'good', 'ones', 'are', 'hard', 'to', 'find', '.', ')', 'somebody', 'did', 'their', 'homework', 'on', 'this', 'one', '--', 'and', 'that', \"'s\", 'what', 'stands', 'out', 'above', 'all', 'else', '.']\n"
     ]
    }
   ],
   "source": [
    "#Pre-process all reviews (extract relevant txt only from dataset, get rid of html tags, pad punctuation with white spaces, tokenize reviews)\n",
    "\n",
    "#take out html tags\n",
    "\n",
    "import re\n",
    "\n",
    "pos[0] = re.sub('<br/><br/>', '', pos[0]) #This only replaces, no space included\n",
    "# print pos[0]\n",
    "\n",
    "#from each txt file, only take out relevant txt - REVIEW itself\n",
    "\n",
    "s = pos[0]\n",
    "result = re.search('>(.*)</span>', s)\n",
    "# print result.group(1)\n",
    "\n",
    "#lowercase all letters\n",
    "\n",
    "review_lowercased = result.group(1).lower()\n",
    "# print review_lowercased\n",
    "\n",
    "# pad punctuation with white spaces\n",
    "\n",
    "padded = review_lowercased\n",
    "padded = re.sub('([.,!?()])', r' \\1 ', padded)\n",
    "padded = re.sub('\\s{2,}', ' ', padded)\n",
    "# print padded\n",
    "\n",
    "#tokenize txt by splitting on white spaces\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_review = word_tokenize(padded)\n",
    "print tokenized_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up Bag of Words Model Feature Extraction\n",
    "\n",
    "def word_feats(tokenized_review):\n",
    "    return dict([(word, True) for word in tokenized_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up features\n",
    "\n",
    "negfeats = [word_feats(tokenized_review) for f in neg]\n",
    "posfeats = [word_feats(tokenized_review) for f in pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 161776 instances, test on 53926 instances\n"
     ]
    }
   ],
   "source": [
    "#Split processed reviews into training and test data sets\n",
    "\n",
    "negcutoff = len(negfeats)*3/4\n",
    "poscutoff = len(posfeats)*3/4\n",
    " \n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c60bb5ffa025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Make Naive Bayes model using training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/mindyng/anaconda2/lib/python2.7/site-packages/nltk/classify/naivebayes.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Count up how many times each feature value occurred, given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# the label and featurename.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mlabel_freqdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "#Make Naive Bayes model using training data\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feed test data into Naive Bayes model and report Naive Bayes Model accuracy\n",
    "\n",
    "print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
    "classifier.show_most_informative_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
